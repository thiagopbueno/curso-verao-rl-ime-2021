{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adolescent-thickness",
   "metadata": {},
   "source": [
    "# Aula 3 - Parte Prática - Problema de Controle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-contamination",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Nesse notebook iremos implementar o algoritmo de controle DQN. Esse será o primeiro algoritmo de Deep RL que veremos no curso, isto é, combinaremos redes neurais e programação dinâmica aproximada. A proposta geral do DQN é combinar a regra de atualização do Q-Learning com aproximadores de função não-lineares para se obter boa generalização sobre o conjunto de possíveis observações.\n",
    "\n",
    "<img src=\"img/control.png\" alt=\"Agent-Env Loop\" style=\"width: 750px;\"/>\n",
    "\n",
    "Na 1a parte desse notebook, nosso objetivo será resolver o ambiente `CartPole-v1`. Já na 2a parte tentaremos encontrar uma solução para o ambiente `PongNoFrameskip-v4` da suite ALE-Atari disponível via OpenAI `gym`.\n",
    "\n",
    "\n",
    "### Objetivos:\n",
    "\n",
    "- Entender o papel da otimalidade de Bellman para algoritmos de controle\n",
    "- Desenvolver intuição sobre o problema de exploração em RL\n",
    "- Ter um primeiro contato com técnicas de treinamento de algoritmos de deep RL\n",
    "- Familiarizar-se com a biblioteca de redes neurais dm-sonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-title",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "\n",
    "É necessário rodar a célula abaixo apenas uma vez para instalar as dependências do notebook. **Atenção**: reinicie o kernel depois de rodar a célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399b488-1f22-4dd8-92b6-2d4f385e74e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atualizar pip\n",
    "!pip install -U pip setuptools\n",
    "# instalar pacotes\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-liberty",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "> Não se esqueça de executar os imports abaixo antes de prosseguir com o notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack, Monitor, TimeLimit\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from utils import logging\n",
    "from utils.nn import initializers\n",
    "from utils import replay\n",
    "from utils import schedule\n",
    "from utils import tf_utils\n",
    "\n",
    "\n",
    "tf_utils.set_tf_allow_growth() # necessário apenas se você dispõe de GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-disco",
   "metadata": {},
   "source": [
    "## Ambiente - CartPole-v1\n",
    "\n",
    "Como veremos no exercício-programa de hoje, é sempre uma boa ideia em aprendizado por reforço iniciar o estudo de um algoritmo por um problema simples e pequeno para o qual você poderá resolver em poucos minutos. Para isso, o ambiente do `CartPole-v1` é usualmente um dos primeiros problemas que um agente baseado em aprendizado por reforço deve ser capaz de resolver antes de tentar atacar problemas mais complexos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_envs(env_id):\n",
    "    env = gym.make(env_id)\n",
    "    eval_env = gym.vector.make(env_id, num_envs=20, asynchronous=True)\n",
    "    test_env = gym.make(env_id)\n",
    "    return env, eval_env, test_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, eval_env, test_env = make_envs(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-serve",
   "metadata": {},
   "source": [
    "> Dica: se você não estiver familiarizado com esse ambiente ou precisar refrescar a memória, lembre-se de consultar a documentação disponível no site do OpenAI Gym [https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control) e também procure entender principalmente os detalhes sobre o espaço de estados e ações do ambiente usando os métodos `env.observation_space` e `env.action_space`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-court",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "\n",
    "Como visto em aula o algoritmo `DQN` procura aproximar a função $Q(s, a)$ utilizando redes neurais treinadas por meio da otimização de uma função objetivo baseada na regra de atualização do Q-Learning. O algoritmo abaixo descreve de maneira geral o treinamento de um agente `DQN`.\n",
    "\n",
    "<img src=\"img/dqn-algo.png\" alt=\"Agent-Env Loop\" style=\"width: 500px;\"/>\n",
    "\n",
    "Nessa seção desenvolveremos os componentes desse algoritmo passo a passo:\n",
    "\n",
    "1. **Redes neurais (networks)**: inicialmente construiremos usando a biblioteca `dm-sonnet` a rede neural para a função $Q(s, a)$;\n",
    "2. **Função objetivo (loss)**: uma vez definida a classe da função $Q(s, a)$, implementaremos a função objetivo utilizada no problema de \"regressão\" que o Q-Learning tenta resolver;\n",
    "3. **Atualização (update)**: em seguida instanciaremos um otimizador baseado em gradientes que será responsável por minimizar a função objetivo previamente definida; e\n",
    "4. **Política $\\epsilon$-greedy**: por fim definiremos a política estocástica para exploração."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-organic",
   "metadata": {},
   "source": [
    "### Redes Neurais (networks)\n",
    "\n",
    "Para representar funções $Q(s,a)$ utilizando redes neurais, temos em geral 2 opções de implementação:\n",
    "1. Definir uma rede com entrada $(s, a)$ e saída um único número real: $Q_\\phi : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$; ou\n",
    "2. definir como entrada apenas o estado $s$ e saída um vetor de tamanho $|\\mathcal{A}|$: $Q_\\phi : \\mathcal{S} \\rightarrow \\mathbb{R}^{|\\mathcal{A}|}$ .\n",
    "\n",
    "Em geral, para algoritmos baseados no `DQN` é costume utilizar a 2a opção.\n",
    "\n",
    "> **Observação**: note que $\\phi \\in \\mathbb{R}^d$ com $d \\ll |S|$, onde $\\phi$ denota o conjunto de parâmetros (i.e., *kernels* e *biases*) da rede neural. Dessa forma, a rede deve extrair apenas informações essenciais sobre o estado para a predição do retorno esperado (como vimos na aula de predição).\n",
    "\n",
    "<img src=\"img/conv-net.png\" alt=\"Agent-Env Loop\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(snt.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, name=\"QNetwork\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # features\n",
    "        self._torso = snt.nets.MLP(\n",
    "            [8, 8],\n",
    "            activation=tf.nn.relu,\n",
    "            activate_final=True,\n",
    "            w_init=initializers.he_initializer(),\n",
    "            name=\"MLP\"\n",
    "        ) \n",
    "\n",
    "        # predictor\n",
    "        self._q_values = snt.Linear(action_space.n, name=\"QValues\")\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, obs):\n",
    "        \"\"\"Calcula os Q-values de todas as ações para uma dada `obs`.\"\"\"\n",
    "        h = self._torso(obs)\n",
    "        return self._q_values(h)\n",
    "\n",
    "    @tf.function\n",
    "    def action_values(self, obs, actions):\n",
    "        \"\"\"Calcula os Q-values de uma única `action` específica para uma dada `obs`.\"\"\"\n",
    "        batch_size = tf.shape(obs)[0]\n",
    "        indices = tf.stack([tf.range(batch_size, dtype=actions.dtype), actions], axis=1)\n",
    "        q_values = tf.gather_nd(self(obs), indices)\n",
    "        return q_values\n",
    "\n",
    "    @tf.function\n",
    "    def hard_update(self, other):\n",
    "        \"\"\"Copia os parâmetros da rede `other` para a rede do objeto.\"\"\"\n",
    "        for self_var, other_var in zip(self.trainable_variables, other.trainable_variables):\n",
    "            self_var.assign(other_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-flood",
   "metadata": {},
   "source": [
    "### Função objetivo (loss)\n",
    "\n",
    "Lembre-se que o `DQN` se utiliza da regra de atualização baseado em programação dinâmica aproximada do Q-Learning:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi) = \\mathbb{E}_{(s, a, r, s') \\sim \\mathcal{D}} [(Q_{\\phi}(s, a) - (r + \\gamma \\max_{a'} Q_{\\bar{\\phi}}(s', a')))^2]\n",
    "$$\n",
    "\n",
    "> **Observação**: lembre que para compor o \"alvo\" da regressão usamos a rede target $Q_{\\bar{\\phi}}$. Conforme vimos na aula teórica, o uso de *target networks* é fundamental para melhorar a estabilidade do treinamento. Caso contrário, toda atualização na direção de $\\nabla_{\\phi} \\mathcal{L}(\\phi)$ acabaria por alterar também o valor do \"alvo\" da regressão, tornando o problema de otimização muito mais complicado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_q_learning_loss(q_net, target_q_net, gamma=0.99):\n",
    "    \"\"\"Recebe a rede online `q_net` e a rede `target_q_net` e devolve o loss function do Q-Learning.\"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def _loss(batch):\n",
    "        \"\"\"Recebe um batch de experiências e devolve o valor da função objetivo para esse batch.\"\"\"\n",
    "        obs = batch[\"obs\"]\n",
    "        actions = batch[\"action\"]\n",
    "        rewards = batch[\"reward\"]\n",
    "        next_obs = batch[\"next_obs\"]\n",
    "        terminals = tf.cast(batch[\"terminal\"], tf.float32)\n",
    "        \n",
    "        # predictions\n",
    "        q_values = q_net.action_values(obs, actions)\n",
    "\n",
    "        # targets\n",
    "        next_q_values = tf.reduce_max(target_q_net(next_obs), axis=-1)\n",
    "        q_targets = tf.stop_gradient(rewards + (1 - terminals) * gamma * next_q_values)\n",
    "\n",
    "        # loss = tf.reduce_mean((q_values - q_targets) ** 2)\n",
    "        loss = tf.losses.huber(q_values, q_targets)\n",
    "        return loss\n",
    "\n",
    "    return _loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-regression",
   "metadata": {},
   "source": [
    "### Atualização (updates)\n",
    "\n",
    "Uma vez com *loss function* definida, basta instanciar um otimizador escolhendo uma taxa de aprendizado (i.e., `learning_rate`) rodando a célula abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_update_fn(loss_fn, trainable_variables, learning_rate=1e-3):\n",
    "    optimizer = snt.optimizers.Adam(learning_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def _update_fn(batch):\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(trainable_variables)\n",
    "            loss = loss_fn(batch)\n",
    "\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply(grads, trainable_variables)\n",
    "\n",
    "        grads_and_vars = {var.name: (grad, var) for grad, var in zip(grads, trainable_variables)}\n",
    "\n",
    "        return loss, grads_and_vars\n",
    "\n",
    "    return _update_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-decrease",
   "metadata": {},
   "source": [
    "### Política $\\epsilon$-*greedy*\n",
    "\n",
    "O último componente do algoritmo `DQN` é sua política utilizada para explorar. No notebook de hoje, implementaremos a política exploratória mais simples.\n",
    "\n",
    "Como vimos na aula teórica, a política $\\epsilon$-*greedy* escolhe uma ação aleatória com probabilidade $\\epsilon$ e escolhe a ação gulosa com probabilidade $1 - \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "\n",
    "    def __init__(self, q_net, start_val=1.0, end_val=0.01, start_step=1_000, end_step=10_000):\n",
    "        self.q_net = q_net\n",
    "\n",
    "        self._schedule = schedule.PiecewiseLinearSchedule((start_step, start_val), (end_step, end_val))\n",
    "\n",
    "        self._step = tf.Variable(0., dtype=tf.float32, name=\"step\")\n",
    "        self._epsilon = tf.Variable(start_val, dtype=tf.float32, name=\"epsilon\")\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        \"\"\"Retorna ação aleatória com probabilidade epsilon, c.c., retorna ação gulosa.\"\"\"\n",
    "        self._epsilon.assign(self._schedule(self._step))\n",
    "        self._step.assign_add(1)\n",
    "\n",
    "        batch_size = tf.shape(obs)[0]\n",
    "        action_dim = self.q_net.action_space.n\n",
    "\n",
    "        random_actions = tf.random.uniform(shape=(batch_size,), minval=0, maxval=action_dim, dtype=tf.int32)\n",
    "        greedy_actions = tf.argmax(self.q_net(obs), axis=-1, output_type=tf.int32)\n",
    "\n",
    "        return tf.where(\n",
    "            self._epsilon > tf.random.uniform(shape=(batch_size,)),\n",
    "            random_actions,\n",
    "            greedy_actions            \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-emerald",
   "metadata": {},
   "source": [
    "### Agente DQN\n",
    "\n",
    "Com todos os componentes definidos, estamos finalmente preparados para instanciar um agente `DQN` para o ambiente `CartPole-v1`. Execute a célula abaixo para criar a classe `DQN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        gamma=0.99,\n",
    "        target_update_freq=1000,\n",
    "        learning_rate=1e-3, \n",
    "        checkpoint_dir=\"ckpt\"\n",
    "    ):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.q_net = QNetwork(self.observation_space, self.action_space, name=\"QNet\")\n",
    "        self.target_q_net = QNetwork(self.observation_space, self.action_space, name=\"TargetQNet\")\n",
    "\n",
    "        self.policy = EpsilonGreedyPolicy(self.q_net)\n",
    "        \n",
    "        self._ckpt_dir = checkpoint_dir\n",
    "        self._ckpt = tf.train.Checkpoint(q_net=self.q_net)\n",
    "        self._ckpt_manager = tf.train.CheckpointManager(self._ckpt, directory=self._ckpt_dir, max_to_keep=1)\n",
    "\n",
    "        self._step = tf.Variable(0, dtype=tf.int32, name=\"step\")\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Cria as variáveis das redes online e target e sincroniza inicialmente.\"\"\"\n",
    "        input_spec = tf.TensorSpec(self.observation_space.shape, dtype=tf.float32)\n",
    "        tf_utils.create_variables(self.q_net, input_spec)\n",
    "        tf_utils.create_variables(self.target_q_net, input_spec)\n",
    "        self.target_q_net.hard_update(self.q_net)\n",
    "\n",
    "    def compile(self):\n",
    "        \"\"\"Compila a DQN loss junto com a Q-network.\"\"\"\n",
    "        self.update_learner = make_update_fn(\n",
    "            make_q_learning_loss(self.q_net, self.target_q_net, gamma=self.gamma),\n",
    "            self.q_net.trainable_variables,\n",
    "            learning_rate=self.learning_rate\n",
    "        )\n",
    "\n",
    "    def step(self, obs, training=True):\n",
    "        \"\"\"Escolhe a ação para a observação dada.\"\"\"\n",
    "        obs = tf.convert_to_tensor(obs, dtype=tf.float32)\n",
    "        action = self.policy(obs) if training else tf.argmax(self.q_net(obs), axis=-1)\n",
    "        return action.numpy()\n",
    "\n",
    "    def learn(self, batch):\n",
    "        \"\"\"Recebe um batch de experiências, atualiza os parâmetros das redes, e devolve algumas métricas.\"\"\"\n",
    "        # atualiza q_net\n",
    "        loss, grads_and_vars = self.update_learner(batch)\n",
    "\n",
    "        # sincroniza target_q_net\n",
    "        self._step.assign_add(1)\n",
    "        if self._step % self.target_update_freq == 0:\n",
    "            self.target_q_net.hard_update(self.q_net)\n",
    "\n",
    "        # métricas de monitoramento\n",
    "        stats = {\n",
    "            \"loss\": loss,\n",
    "            \"q_values_mean\": tf.reduce_mean(self.q_net(batch[\"obs\"])),\n",
    "            \"epsilon\": self.policy._epsilon,\n",
    "            \"vars\": {key: variable for key, (_, variable) in grads_and_vars.items()},\n",
    "            \"grads\": {f\"grad_{key}\": grad for key, (grad, _) in grads_and_vars.items()},\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Salva o estado atual do agente (i.e., o valor dos parâmetros da rede online) nesse momento.\"\"\"\n",
    "        return self._ckpt_manager.save()\n",
    "\n",
    "    def restore(self, save_path=None):\n",
    "        \"\"\"Carrega o último checkpoint salvo anteriormente no `save_path`.\"\"\"\n",
    "        if not save_path:\n",
    "            save_path = self._ckpt_manager.latest_checkpoint\n",
    "        return self._ckpt.restore(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-navigation",
   "metadata": {},
   "source": [
    "## Protocolo de treinamento, avaliação e teste\n",
    "\n",
    "Com a classe do `DQN` definida é hora de treinar o agente e avaliá-lo. Faremos isso seguindo um protocolo de treinamento e avaliação definido pelas funções `train` e  `evaluate` abaixo.\n",
    "\n",
    "<img src=\"img/rl-training.png\" alt=\"Agent-Env Loop\" style=\"width: 850px;\"/>\n",
    "\n",
    "Tente entender como os hiperparâmetros de início de treinamento `learning_starts` e frequência de atualizações `learn_every` e avaliação `evaluation_freq` definem o protocolo.\n",
    "\n",
    "> **Observação**: note que embora o protocolo abaixo seja bastante genérico, tenha em mente que diferentes trabalhos alteram a maneira como os processos de coleta de dados, aprendizado e avaliação se intercalam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    agent,\n",
    "    env,\n",
    "    test_env,\n",
    "    replay,\n",
    "    logger,\n",
    "    total_timesteps=20_000,\n",
    "    learning_starts=1_000,\n",
    "    learn_every=1,\n",
    "    evaluation_freq=1_000\n",
    "):  \n",
    "    timesteps = 0\n",
    "    episodes = 0\n",
    "    episode_returns = deque(maxlen=100)\n",
    "\n",
    "    best_episode_reward_mean = -np.inf\n",
    "    \n",
    "    with trange(total_timesteps, desc=\"training\") as pbar:\n",
    "\n",
    "        while timesteps < total_timesteps:\n",
    "            obs = env.reset()\n",
    "            episode_return = 0.0\n",
    "\n",
    "            for episode_length in range(1, env.spec.max_episode_steps + 1):\n",
    "\n",
    "                # collect\n",
    "                action = agent.step(np.expand_dims(obs, axis=0), training=True)[0]\n",
    "                next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "                timesteps += 1\n",
    "                episode_return += reward\n",
    "\n",
    "                # add experience to replay buffer\n",
    "                terminal = done if episode_length < env.spec.max_episode_steps else False\n",
    "                replay.add(obs, action, reward, terminal, next_obs)\n",
    "\n",
    "                # training\n",
    "                if timesteps >= learning_starts and timesteps % learn_every == 0:\n",
    "                    batch = replay.sample()\n",
    "                    stats = agent.learn(batch)\n",
    "                    stats[\"episode_return_mean\"] = np.mean(episode_returns)\n",
    "                    logger.log(timesteps, stats, label=\"train\") # logging\n",
    "\n",
    "                # evaluation\n",
    "                if timesteps % evaluation_freq == 0:\n",
    "                    stats = evaluate(agent, test_env)\n",
    "                    logger.log(timesteps, stats, label=\"evaluation\") # logging\n",
    "\n",
    "                    # checkpointing\n",
    "                    if stats[\"episode_return_mean\"] > best_episode_reward_mean:\n",
    "                        agent.save()\n",
    "                        best_episode_reward_mean = stats[\"episode_return_mean\"]\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                obs = next_obs\n",
    "\n",
    "            episodes += 1\n",
    "            episode_returns.append(episode_return)\n",
    "\n",
    "            # logging\n",
    "            stats = {\n",
    "                \"episodes\": episodes,\n",
    "                \"episode_length\": episode_length,\n",
    "                \"episode_return\": episode_return,\n",
    "            }\n",
    "            logger.log(timesteps, stats, label=\"collect\")\n",
    "            logger.flush()\n",
    "\n",
    "            pbar.update(episode_length)\n",
    "            pbar.set_postfix(timesteps=timesteps, episodes=episodes, avg_returns=np.mean(episode_returns) if episode_returns else None)\n",
    "\n",
    "    # final evaluation\n",
    "    stats = evaluate(agent, test_env)\n",
    "    logger.log(timesteps, stats, label=\"evaluation\")\n",
    "    logger.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-tablet",
   "metadata": {},
   "source": [
    "Para avaliarmos o agente, utilizaremos o `eval_env` que foi criado como um ambiente paralelizado (contendo `env.num_envs` rodando de forma assíncrona em paralelo). \n",
    "\n",
    "> **Observação**: Note no código abaixo, como esse tipo de ambiente altera ligeiramente o ciclo de interação agente-ambiente que vimos nas últimas aulas. Para maiores detalhes, consulte a documentação de `gym.vector.make` e o código dos módulos em [https://github.com/openai/gym/tree/master/gym/vector](https://github.com/openai/gym/tree/master/gym/vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env):\n",
    "    total_reward = np.zeros((env.num_envs,))\n",
    "    episode_length = np.zeros((env.num_envs,))\n",
    "\n",
    "    obs = env.reset()\n",
    "    dones = np.array([False] * env.num_envs)\n",
    "\n",
    "    while not np.all(dones):\n",
    "        action = agent.step(obs, training=False)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += (1 - dones) * reward\n",
    "        episode_length += (1 - dones)\n",
    "        dones = np.logical_or(dones, done)\n",
    "\n",
    "    return {\n",
    "        \"episode_return_mean\": np.mean(total_reward),\n",
    "        \"episode_return_min\": np.min(total_reward),\n",
    "        \"episode_return_max\": np.max(total_reward),\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-plain",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para definir um ciclo de interação agente-ambiente para renderizar episódios do agente após o treinamento e então verificar qualitativamente quão boa foi a política que o agente aprendeu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, env, episodes=3, wait=None):\n",
    "    for episode in range(episodes):\n",
    "        obs = env.reset()\n",
    "        env.render()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.step(np.expand_dims(obs, axis=0), training=False)[0]\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            if wait:\n",
    "                time.sleep(wait)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-reservoir",
   "metadata": {},
   "source": [
    "## Treinando DQN no CartPole-v1\n",
    "\n",
    "Finalmente, temos todo o código necessário para treinarmos o `DQN` no `CartPole-v1`. Antes de iniciarmos o treinamento, execute a célula abaixo para instanciarmos o `tensorboard`, a ferramenta de *logging* e monitoramento do TensorFlow. Consulte a documentação e os tutoriais disponíveis em [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard) para maiores informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --reload_interval 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, total_timesteps=20_000, trials=3):\n",
    "    for _ in range(trials):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n",
    "        run_id = osp.join(f\"dqn-{env.spec.id}\".lower(), timestamp)\n",
    "\n",
    "        logger = logging.TFLogger(run_id, base_dir=\"logs\")\n",
    "\n",
    "        buffer = replay.ReplayBuffer(env.observation_space, env.action_space, max_size=total_timesteps, batch_size=64)\n",
    "        buffer.build()\n",
    "\n",
    "        agent = DQN(env.observation_space, env.action_space, checkpoint_dir=f\"ckpt/{run_id}\")\n",
    "        agent.build()\n",
    "        agent.compile()\n",
    "\n",
    "        train(agent, env, eval_env, buffer, logger, total_timesteps=total_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(env, trials=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-tours",
   "metadata": {},
   "source": [
    "Agora é só buscar um café esperar o resultado do treinamento. ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-multiple",
   "metadata": {},
   "source": [
    "### Teste do Agente no CartPole-v1\n",
    "\n",
    "Escolha um dos agentes treinados acima para testar e visualizar seu comportamento com o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"ckpt/dqn-cartpole-v1/2021-01-28-01:49\" # altere essa linha para escolher qual checkpoint do agente\n",
    "\n",
    "agent = DQN(env.observation_space, env.action_space, checkpoint_dir=checkpoint_dir)\n",
    "agent.build()\n",
    "agent.restore()\n",
    "\n",
    "test(agent, test_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-setting",
   "metadata": {},
   "source": [
    "## DQN no Atari (Pong)\n",
    "\n",
    "Nesta 2a parte do notebook, implementaremos algumas melhorias no agente do `DQN` para treinarmos um agente para o ambiente do `PongNoFrameskip-v4`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f77e0-70ee-4d31-8cb0-7ad43824fc0b",
   "metadata": {},
   "source": [
    "### Dependências do Atari\n",
    "\n",
    "Simular jogos de Atari no OpenAI Gym requer ROMs distribuídos separadamente, como descrito na [documentação](https://github.com/openai/atari-py#roms).\n",
    "\n",
    "O script abaixo baixa os arquivos necessários caso você não tenha os ROMs no diretório local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c99379-6011-422b-980b-928db66f77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "if not osp.exists(\"Roms.rar\"):\n",
    "    urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43879f54-d183-4831-84d8-0c43827cccd2",
   "metadata": {},
   "source": [
    "Você precisará descompactar o arquivo `Roms.rar`, procedimento que varia de acordo com o sistema operacional.\n",
    "\n",
    "Para usuários do Linux, recomendamos instalar o pacote `unrar`. No Ubuntu, é possível fazer isso via `apt`:\n",
    "```\n",
    "supo apt install unrar\n",
    "```\n",
    "Descompacte o conteúdo de `Roms.rar` em uma pasta `roms/` no diretório local. Com o `unrar`, isso é feito pelo comando\n",
    "```\n",
    "unrar e Roms.rar roms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b46f4a-0ecf-4fd6-95ef-fb11bb17cd8b",
   "metadata": {},
   "source": [
    "Com o conteúdo descompactado, execute a célula abaixo para configurar o `gym` para usá-lo nas simulações do Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a652ef08-eb55-4f70-9c71-93ad2e48d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert osp.exists(\"roms\")\n",
    "!python -m atari_py.import_roms roms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-relation",
   "metadata": {},
   "source": [
    "### Ambiente - PongNoFrameskip-v4\n",
    "\n",
    "Para treinar o `DQN` na versão original do artigo (Mnih et al, 2015) para os jogos do Atari, precisamos aplicar algumas transformações em cima do ambiente nativo do simulare ALE encapsulado via pacote `gym`. Note o uso dos `gym.wrappers` na construção do ambiente no código abaixo.\n",
    "\n",
    "> **Observação**: o aluno interessado em resolver outros jogos do Atari deve consular o artigo [Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents](https://www.jair.org/index.php/jair/article/download/11182/26388) para maiores detalhes sobre o uso de `wrappers` e transformações do ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_envs(env_id):\n",
    "    assert \"NoFrameskip\" in env_id\n",
    "\n",
    "    def _wrapper(env, max_episode_steps=None, num_stack=4, terminal_on_life_loss=False):\n",
    "        # limita o número máximo de passos de interação em um episódio\n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "        \n",
    "        # diminue o tamanho da imagem, muda de RGB para greyscale, adiciona action repeats e frame_skip\n",
    "        env = AtariPreprocessing(env=env, frame_skip=4, terminal_on_life_loss=terminal_on_life_loss)\n",
    "        \n",
    "        # concatena frames consecutimos como observação -- importante para tentar aproximar a propriedade de Markov do estado\n",
    "        env = FrameStack(env=env, num_stack=num_stack)\n",
    "        return env\n",
    "\n",
    "    env = _wrapper(gym.make(env_id), max_episode_steps=50_000, terminal_on_life_loss=True)\n",
    "    eval_env = gym.vector.make(env_id, num_envs=10, asynchronous=True, wrappers=lambda env: _wrapper(env, max_episode_steps=108_000))\n",
    "    test_env = _wrapper(gym.make(env_id), max_episode_steps=108_000)\n",
    "\n",
    "    return env, eval_env, test_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "env, eval_env, test_env = make_envs(\"PongNoFrameskip-v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-grain",
   "metadata": {},
   "source": [
    "### Double Q-Learning\n",
    "\n",
    "É comum um agente baseado no algoritmo do `DQN` superestimar a função ao longo do tempo de treinamento, mesmo com o uso de *target networks*. Diferentemente do ambiente do `CartPole-v1`, em jogos de Atari é preciso centenas de milhares ou mesmo milhões de `timesteps` para convergir um agente `DQN`. Dessa forma esse problema de super-estimação dos valores de $Q(s, a)$ pode contribuir para instabilidade e consequente divergência do treinamento.\n",
    "\n",
    "O problema de super-estimação é decorrente da aproximação que fazemos no Q-Learning:\n",
    "$$\n",
    "\\mathbb{E}_{s' \\sim p(\\cdot|s, a)} [\\max_{a'} \\tilde{Q}(s', a')] \\approx \\max_{a'} \\tilde{Q}(s', a')~,\n",
    "$$\n",
    "onde usamos apenas uma única amostra para estimar o valor esperado.\n",
    "\n",
    "**Observação**: para o aluno interessado consulte o artigo [Deep reinforcement learning with double q-learning](https://ojs.aaai.org/index.php/AAAI/article/download/10295/10154).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-fields",
   "metadata": {},
   "source": [
    "A solução encontrada se chama *Double Q-Learning* na qual alteramos ligeiramente a maneira como calculamos os Q-values para o *target* da regressão:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi) = \\mathbb{E}_{(s, a, r, s') \\sim \\mathcal{D}} [(Q_{\\phi}(s, a) - (r + \\gamma Q_{\\bar{\\phi}}(s', \\arg\\max_{a'} Q_{\\phi}(s', a'))))^2]\n",
    "$$\n",
    "\n",
    "> **Observação**: note que a única alteração é usar a rede \"online\" $Q_\\phi$ para escolher a melhor ação no próximo estado $s'$, mas continuar a avaliar com a rede \"target\" $Q_{\\bar{\\phi}}$ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_double_q_learning_loss(q_net, target_q_net, gamma=0.99):\n",
    "    \"\"\"Recebe a rede online `q_net` e a rede `target_q_net` e devolve o loss function do Double Q-Learning.\"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def _loss(batch):\n",
    "        \"\"\"Recebe um batch de experiências e devolve o valor da função objetivo para esse batch.\"\"\"\n",
    "        obs = batch[\"obs\"]\n",
    "        actions = batch[\"action\"]\n",
    "        rewards = tf.clip_by_value(batch[\"reward\"], -1., 1.)\n",
    "        next_obs = batch[\"next_obs\"]\n",
    "        terminals = tf.cast(batch[\"terminal\"], tf.float32)\n",
    "\n",
    "        # predictions\n",
    "        q_values = q_net.action_values(obs, actions)\n",
    "\n",
    "        # targets\n",
    "        next_actions = tf.argmax(q_net(next_obs), axis=-1, output_type=tf.int32)\n",
    "        next_q_values = target_q_net.action_values(next_obs, next_actions)\n",
    "        q_targets = tf.stop_gradient(rewards + (1 - terminals) * gamma * next_q_values)\n",
    "\n",
    "        # loss = tf.reduce_mean((q_values - q_targets) ** 2)\n",
    "        loss = tf.losses.huber(q_values, q_targets)\n",
    "        return loss\n",
    "\n",
    "    return _loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-angel",
   "metadata": {},
   "source": [
    "### Dueling QNetwork\n",
    "\n",
    "A última modificação do `DQN` que precisamos são as chamadas *Dueling Networks*. A ideia básica é introduzir estrutura na rede neural que tenta predizer os valores de $Q(s, a)$.\n",
    "\n",
    "Para isso definimos a chamada *Advantage function* (não vista na aula teórica ainda):\n",
    "$$\n",
    "A^{\\pi}(s, a) = Q^\\pi(s, a) - V^\\pi(s)\n",
    "$$\n",
    "\n",
    "A função *advantage* estima o quão melhor é uma ação com relação ao valor médio sobre todas as ações (lembre-se que $V^\\pi(s) = \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} Q^\\pi(s, a)$).\n",
    "\n",
    "Note que $\\mathbb{E}_{a \\sim \\pi(\\cdot|s)} A^{\\pi}(s, a) = \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [Q^\\pi(s, a) - V^\\pi(s)] = \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [Q^\\pi(s, a)] - V^\\pi(s) = V^\\pi(s) - V^\\pi(s) = 0$, isto é, como esperado a função vantagem tem média zero sobre as ações!\n",
    "\n",
    "Dessa forma, podemos decompor $Q(s, a)$ como a soma de uma componente de média zero com a função Valor do estado:\n",
    "$$\n",
    "Q^\\pi(s, a)  = A^{\\pi}(s, a) + V^\\pi(s)\n",
    "$$\n",
    "\n",
    "<img src=\"img/dueling-q-net.png\" alt=\"Agent-Env Loop\" style=\"width: 650px;\"/>\n",
    "\n",
    "Na prática, essa estrutura pode facilar o aprendizado da função para problemas em que em certas situações é mais fácil prever diferenças entre ações do que estimar o retorno propriamente dita!\n",
    "\n",
    "> **Observação**: o aluno interessado pode consular o artigo [Dueling network architectures for deep reinforcement learning](http://proceedings.mlr.press/v48/wangf16.pdf) para maiores detalhes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQNetwork(snt.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, name=\"AtariQNetwork\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Atari torso\n",
    "        self._torso = snt.Sequential([\n",
    "            snt.Conv2D(32, kernel_shape=8, stride=4, padding=\"VALID\", w_init=initializers.he_initializer(), name=\"Conv1\"),\n",
    "            tf.nn.relu,\n",
    "            snt.Conv2D(64, kernel_shape=4, stride=2, padding=\"VALID\", w_init=initializers.he_initializer(), name=\"Conv2\"),\n",
    "            tf.nn.relu,\n",
    "            snt.Conv2D(64, kernel_shape=3, stride=1, padding=\"VALID\", w_init=initializers.he_initializer(), name=\"Conv3\"),\n",
    "            tf.nn.relu,\n",
    "            snt.Flatten(),\n",
    "        ])\n",
    "\n",
    "        # predictors (dueling network)\n",
    "        self._value_mlp = snt.nets.MLP([512, 1], w_init=initializers.he_initializer(), activation=tf.nn.relu, activate_final=False, name=\"Value\")\n",
    "        self._advantage_mlp = snt.nets.MLP([512, action_space.n], w_init=initializers.he_initializer(), activation=tf.nn.relu, activate_final=False, name=\"Advantage\")\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, obs):\n",
    "        \"\"\"Calcula os Q-values de todas as ações para uma dada `obs`.\"\"\"\n",
    "        # pre-processamento\n",
    "        obs = tf.cast(obs, dtype=tf.float32) / 255.\n",
    "        obs = tf.transpose(obs, perm=[0, 2, 3, 1])\n",
    "\n",
    "        # features\n",
    "        h = self._torso(obs)\n",
    "\n",
    "        # predições\n",
    "        values = self._value_mlp(h)\n",
    "        advantages = self._advantage_mlp(h)\n",
    "        advantages -= tf.reduce_mean(advantages, axis=-1, keepdims=True)\n",
    "        q_values = values + advantages\n",
    "\n",
    "        return q_values\n",
    "\n",
    "    @tf.function\n",
    "    def action_values(self, obs, actions):\n",
    "        \"\"\"Calcula os Q-values de uma única `action` específica para uma dada `obs`.\"\"\"\n",
    "        batch_size = tf.shape(obs)[0]\n",
    "        indices = tf.stack([tf.range(batch_size, dtype=actions.dtype), actions], axis=1)\n",
    "        q_values = tf.gather_nd(self(obs), indices)\n",
    "        return q_values\n",
    "\n",
    "    @tf.function\n",
    "    def hard_update(self, other):\n",
    "        \"\"\"Copia os parâmetros da rede `other` para a rede do objeto.\"\"\"\n",
    "        for self_var, other_var in zip(self.trainable_variables, other.trainable_variables):\n",
    "            self_var.assign(other_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-blake",
   "metadata": {},
   "source": [
    "### Agente Double DQN (DDQN)\n",
    "\n",
    "Agora é só juntar os novos componentes no agente `DDQN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        gamma=0.99,\n",
    "        target_update_freq=1000,\n",
    "        learning_rate=2.5e-4,\n",
    "        checkpoint_dir=\"ckpt\"\n",
    "    ):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.q_net = DuelingQNetwork(self.observation_space, self.action_space, name=\"QNet\")\n",
    "        self.target_q_net = DuelingQNetwork(self.observation_space, self.action_space, name=\"TargetQNet\")\n",
    "\n",
    "        self.policy = EpsilonGreedyPolicy(self.q_net, start_val=1.0, end_val=0.01, start_step=10_000, end_step=250_000)\n",
    "\n",
    "        self._ckpt_dir = checkpoint_dir\n",
    "        self._ckpt = tf.train.Checkpoint(q_net=self.q_net)\n",
    "        self._ckpt_manager = tf.train.CheckpointManager(self._ckpt, directory=self._ckpt_dir, max_to_keep=1)\n",
    "\n",
    "        self._step = tf.Variable(0, dtype=tf.int32, name=\"step\")\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Cria as variáveis das redes online e target e sincroniza inicialmente.\"\"\"\n",
    "        input_spec = tf.TensorSpec(self.observation_space.shape, dtype=tf.float32)\n",
    "        tf_utils.create_variables(self.q_net, input_spec)\n",
    "        tf_utils.create_variables(self.target_q_net, input_spec)\n",
    "        self.target_q_net.hard_update(self.q_net)\n",
    "\n",
    "    def compile(self):\n",
    "        \"\"\"Compila a Double DQN loss junto com a DuelingQNetwork.\"\"\"\n",
    "        self.update_learner = make_update_fn(\n",
    "            make_double_q_learning_loss(self.q_net, self.target_q_net, gamma=self.gamma),\n",
    "            self.q_net.trainable_variables,\n",
    "            learning_rate=self.learning_rate\n",
    "        )\n",
    "\n",
    "    def step(self, obs, training=True):\n",
    "        \"\"\"Escolhe a ação para a observação dada.\"\"\"\n",
    "        obs = tf.convert_to_tensor(obs, dtype=tf.float32)\n",
    "        action = self.policy(obs) if training else tf.argmax(self.q_net(obs), axis=-1)\n",
    "        return action.numpy()\n",
    "\n",
    "    def learn(self, batch):\n",
    "        \"\"\"Recebe um batch de experiências, atualiza os parâmetros das redes, e devolve algumas métricas.\"\"\"\n",
    "        loss, grads_and_vars = self.update_learner(batch)\n",
    "\n",
    "        # update target network\n",
    "        self._step.assign_add(1)\n",
    "        if self._step % self.target_update_freq == 0:\n",
    "            self.target_q_net.hard_update(self.q_net)\n",
    "\n",
    "        stats = {\n",
    "            \"loss\": loss,\n",
    "            #\"q_values_mean\": tf.reduce_mean(self.q_net(batch[\"obs\"])),\n",
    "            \"epsilon\": self.policy._epsilon,\n",
    "            \"vars\": {key: variable for key, (_, variable) in grads_and_vars.items()},\n",
    "            \"grads\": {f\"grad_{key}\": grad for key, (grad, _) in grads_and_vars.items()},\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Salva o estado atual do agente (i.e., o valor dos parâmetros da rede online) nesse momento.\"\"\"\n",
    "        return self._ckpt_manager.save()\n",
    "\n",
    "    def restore(self, save_path=None):\n",
    "        \"\"\"Carrega o último checkpoint salvo anteriormente no `save_path`.\"\"\"\n",
    "        if not save_path:\n",
    "            save_path = self._ckpt_manager.latest_checkpoint\n",
    "        return self._ckpt.restore(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-pontiac",
   "metadata": {},
   "source": [
    "### Treinando DDQN no Pong (do zero)\n",
    "\n",
    "Execute a célula abaixo para definir o protocolo de treinamento e avaliação próprio para jogos de Atari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    agent, \n",
    "    env, \n",
    "    test_env, \n",
    "    replay,\n",
    "    logger,\n",
    "    total_timesteps=500_000, \n",
    "    learning_starts=2_500, \n",
    "    learn_every=1, \n",
    "    evaluation_freq=5_000\n",
    "):\n",
    "    timesteps = 0\n",
    "    episodes = 0\n",
    "    episode_returns = deque(maxlen=20)\n",
    "\n",
    "    while timesteps < total_timesteps:\n",
    "\n",
    "        obs = env.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        for episode_length in range(1, env.spec.max_episode_steps + 1):\n",
    "\n",
    "            # collect\n",
    "            action = agent.step(np.expand_dims(obs, axis=0), training=True)[0]\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            timesteps += 1\n",
    "            episode_return += reward\n",
    "\n",
    "            # add experience to replay buffer\n",
    "            terminal = done if episode_length < env.spec.max_episode_steps else False\n",
    "            replay.add(obs, action, reward, terminal, next_obs)\n",
    "\n",
    "            # training\n",
    "            if timesteps >= learning_starts and timesteps % learn_every == 0:\n",
    "                batch = replay.sample()\n",
    "                train_stats = agent.learn(batch)\n",
    "\n",
    "            # evaluation\n",
    "            if timesteps % evaluation_freq == 0:\n",
    "                eval_stats = evaluate(agent, eval_env)\n",
    "\n",
    "                # logging\n",
    "                train_stats[\"episode_reward_mean\"] = np.mean(episode_returns)\n",
    "                logger.log(timesteps, train_stats, label=\"train\")\n",
    "                logger.log(timesteps, eval_stats, label=\"evaluation\")\n",
    "\n",
    "                # checkpointing\n",
    "                agent.save()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        episodes += 1\n",
    "        episode_returns.append(episode_return)\n",
    "\n",
    "        # logging\n",
    "        stats = {\n",
    "            \"episodes\": episodes,\n",
    "            \"episode_length\": episode_length,\n",
    "            \"episode_return\": episode_return,\n",
    "        }\n",
    "        logger.log(timesteps, stats, label=\"collect\")\n",
    "        logger.flush()\n",
    "\n",
    "        print(f\"Timesteps = {timesteps:5d} | Episodes = {episodes:4d} | Episode Length = {episode_length:4d} | Episode Return = {episode_return:.3f}\")\n",
    "\n",
    "    # final evaluation\n",
    "    stats = evaluate(agent, eval_env)\n",
    "    logger.log(timesteps, stats, label=\"evaluation\")\n",
    "    logger.flush()\n",
    "\n",
    "    agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-torture",
   "metadata": {},
   "source": [
    "Na célula abaixo, criamos o `AtariReplayBuffer` e o agente `DDQN`.\n",
    "\n",
    "> **IMPORTANTE**: para armazenar 500K timesteps no buffer será necessário aproximadamente 4G de memória RAM. Se você tiver limitações de memória diminua para 200K ou menos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n",
    "run_id = osp.join(f\"ddqn-{env.spec.id}\".lower(), timestamp)\n",
    "\n",
    "logger = logging.TFLogger(run_id, base_dir=\"logs\")\n",
    "\n",
    "buffer = replay.AtariReplayBuffer(env.observation_space, env.action_space, max_size=500_000, batch_size=32)\n",
    "buffer.build()\n",
    "\n",
    "agent = DDQN(env.observation_space, env.action_space, checkpoint_dir=f\"ckpt/{run_id}\")\n",
    "agent.build()\n",
    "agent.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-berlin",
   "metadata": {},
   "source": [
    "> **Observação**: se você quiser treinar o agente do zero, isto é, extrator de features da rede convolucional e o preditor em cima das features, execute o código abaixo. O treinamento para 500K timesteps deve demorar algumas horas dependendo do seu hardware. Tipicamente, em GPUs deve ser necessário algo entre 1h e 3h. Para treinamento exclusivamente em CPUs, o treinamento deve demorar algo em torno de mais de 12h. Fique atento à isso, pois o seu computador pode super-aquecer se você não estiver preparado para deixar o computador rodando por tanto tempo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(agent, env, eval_env, buffer, logger, total_timesteps=500_000, learning_starts=2_500, evaluation_freq=5_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-riverside",
   "metadata": {},
   "source": [
    "### Treinando DDQN no Pong (com features pré-treinadas)\n",
    "\n",
    "Se você quiser treinar apenas as camadas de predição da `DuelingQNetwork`, pode reaproveitar o extrator de features do agente pré-treinado disponível em `ckpt/ddqn-pongnoframeskip-v4/2021-01-19-16:30/ckpt-101`. Isso deve provavelmente acelerar o treinamento em CPUs em algumas horas.\n",
    "\n",
    "Execute as próximas células."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_trained_agent(run_id, checkpoint_dir, features_only=True):\n",
    "    trained_agent = DDQN(env.observation_space, env.action_space, checkpoint_dir=f\"ckpt/{run_id}\")\n",
    "    trained_agent.build()\n",
    "    trained_agent.restore(checkpoint_dir).assert_consumed()\n",
    "\n",
    "    if features_only:\n",
    "        # transfer learning\n",
    "        trainable_variables = []\n",
    "        for online_var, target_var in zip(trained_agent.q_net.trainable_variables, trained_agent.target_q_net.trainable_variables):\n",
    "            if \"Conv\" in online_var.name: # Conv layers\n",
    "                target_var.assign(online_var)\n",
    "            else: # MLP layers\n",
    "                online_var.assign(target_var)\n",
    "                trainable_variables.append(online_var)\n",
    "\n",
    "        trained_agent.update_learner = make_update_fn(\n",
    "            make_double_q_learning_loss(trained_agent.q_net, trained_agent.target_q_net, gamma=trained_agent.gamma),\n",
    "            trainable_variables,\n",
    "            learning_rate=trained_agent.learning_rate\n",
    "        )\n",
    "\n",
    "    return trained_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n",
    "run_id = osp.join(f\"ddqn-{env.spec.id}\".lower(), timestamp)\n",
    "\n",
    "logger = logging.TFLogger(run_id, base_dir=\"logs\")\n",
    "\n",
    "buffer = replay.AtariReplayBuffer(env.observation_space, env.action_space, max_size=500_000, batch_size=32)\n",
    "buffer.build()\n",
    "\n",
    "checkpoint_dir = \"ckpt/ddqn-pongnoframeskip-v4/2021-01-19-16:30/ckpt-101\"\n",
    "agent = load_pre_trained_agent(run_id, checkpoint_dir, features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(agent, env, eval_env, buffer, logger, total_timesteps=500_000, learning_starts=2_500, evaluation_freq=5_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-maximum",
   "metadata": {},
   "source": [
    "### Teste do Agente no PongNoFrameskip-v4\n",
    "\n",
    "Por fim, escolha o agente `DDQN` treinado acima para testar e visualizar seu comportamento com o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_dir = \"ckpt/ddqn-pongnoframeskip-v4/2021-01-19-16:30\" # altere essa linha para escolher qual checkpoint do agente\n",
    "checkpoint_dir = \"ckpt/ddqn-pongnoframeskip-v4/2021-01-19-21:23\"\n",
    "\n",
    "agent = DDQN(env.observation_space, env.action_space, checkpoint_dir=checkpoint_dir)\n",
    "agent.build()\n",
    "agent.restore()\n",
    "\n",
    "test(agent, test_env, episodes=1, wait=0.02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
