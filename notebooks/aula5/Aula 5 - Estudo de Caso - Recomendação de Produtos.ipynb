{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "foreign-nitrogen",
   "metadata": {},
   "source": [
    "# Aula 5 - Estudo de Caso - Recomendação de Produtos\n",
    "\n",
    "Cada vez mais as pessoas estão realizando pedidos de compras online (até mesmo em função da pandemia). Algumas empresas de supermercados têm tentado desenvolver sistemas de recomendação para facilitar a interação do usuário com o site de compras. Imagine o cenário em que um usuário retorna frequentemente ao sistema e deseja ter o carrinho já montado com suas preferências usuais.\n",
    "\n",
    "<img src=\"img/cart.png\" alt=\"shopping-cart\" style=\"width: 450px;\"/>\n",
    "\n",
    "Nesse estudo de caso, imagine que você é um engenheiro de *machine learning* com a tarefa de automatizar a recomendação de itens no carrinho de compra dos clientes do site. Você tem acesso a um *dataset* de dados históricos e um simulador construído em cima desses dados.\n",
    "\n",
    "A metodologia utilizado nesse estudo de caso será dividida nas seguintes 4 etapas:\n",
    "\n",
    "1. **Entendendo o simulador (caixa-preta)**: quais as características gerais do simulador? como é o espaço de observações e de ações?\n",
    "2. **Escolhendo o algoritmo**: dentre os algoritmos estudados em aula, qual o mais adequado? E por quê? É necessário alterar o algoritmo de alguma forma?\n",
    "3. **Análise da solução**: qualitativamente como podemos avaliar o desempenho da solução obtida?\n",
    "4. **Decisões de modelagem - MDPs (caixa-branca)**: quais as principais escolhas de modelagem usadas no simulador? É possível melhorar?\n",
    "\n",
    "\n",
    "### Objetivos:\n",
    "\n",
    "\n",
    "- Estudar uma primeira abordagem para problemas de tomada de decisão com dados históricos\n",
    "- Analisar e criticar escolhas de modelagem do ambiente\n",
    "- Adaptar algoritmos clássicos de Deep RL para aplicações específicas\n",
    "- Implementar soluções para espaço combinatório de de ações\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- [Instacart](https://www.instacart.com/)\n",
    "- [3 Million Instacart Orders, Open Sourced](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2)\n",
    "- [winderresearch/rl/gym-shopping-cart](https://gitlab.com/winderresearch/rl/gym-shopping-cart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-defensive",
   "metadata": {},
   "source": [
    "### Instalação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N https://s3.eu-west-2.amazonaws.com/assets.winderresearch.com/data/instacart_online_grocery_shopping_2017_05_01.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym-shopping-cart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-honor",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, defaultdict\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import os.path as osp\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import gym_shopping_cart\n",
    "from gym_shopping_cart.data.parser import InstacartData\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from utils import logging\n",
    "from utils.nn import initializers\n",
    "from utils import replay\n",
    "from utils import schedule\n",
    "from utils import tf_utils\n",
    "from utils import wrapper\n",
    "\n",
    "\n",
    "tf_utils.set_tf_allow_growth() # necessário apenas se você dispõe de GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-battle",
   "metadata": {},
   "source": [
    "# Introdução ao problema de recomendação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "gz_file = pathlib.Path(\"instacart_online_grocery_shopping_2017_05_01.tar.gz\")\n",
    "data = InstacartData(gz_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produtos\n",
    "print(f\"# products  = {data.n_products()}\")\n",
    "\n",
    "# usuários\n",
    "all_orders = data._merged_data()\n",
    "all_users = all_orders[\"user_id\"].unique()\n",
    "print(f\"# users     = {len(all_users)}\")\n",
    "\n",
    "# pedidos\n",
    "df = data._orders()\n",
    "print(f\"# orders    = {len(df)}\")\n",
    "\n",
    "# compras (número de produtos vendidos no período)\n",
    "df = data._prior_products()\n",
    "print(f\"# purchases = {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-counter",
   "metadata": {},
   "source": [
    "# 1. Entendendo o simulador (caixa-preta)\n",
    "\n",
    "Quais as características gerais do simulador? Como é o espaço de observações e de ações?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_shopping_cart_envs(max_products=5, user_id=None):\n",
    "    data = InstacartData(gz_file, max_products=max_products)\n",
    "    env_id = \"ShoppingCart-v0\"\n",
    "    train_env = gym.make(env_id, data=data, user_id=user_id)\n",
    "    eval_env = gym.make(env_id, data=data, user_id=user_id)\n",
    "    test_env = wrapper.ShoppingCartWrapper(gym.make(env_id, data=data, user_id=user_id))\n",
    "    if user_id:\n",
    "        for env in [train_env, eval_env, test_env]:\n",
    "            env.spec.id += f\"-user_id={user_id}\"\n",
    "    return train_env, eval_env, test_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_episode(env):\n",
    "    episode_length, episode_return = 0, 0.0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_length += 1\n",
    "        episode_return += reward\n",
    "    return episode_length, episode_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-background",
   "metadata": {},
   "source": [
    "A célula abaixo implementa uma função que avalia uma política uniforme. Esse é um primeiro _baseline_ interessante para avaliarmos ao explorar novos problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_random_returns(env, episodes):\n",
    "    episode_lengths, episode_returns = [], []\n",
    "\n",
    "    for _ in trange(episodes):\n",
    "        episode_length, episode_return = sample_random_episode(env)\n",
    "        episode_lengths.append(episode_length)\n",
    "        episode_returns.append(episode_return)\n",
    "\n",
    "    episode_length_mean = np.mean(episode_lengths)\n",
    "    episode_return_mean = np.mean(episode_returns)\n",
    "    return episode_length_mean, episode_return_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_agent(env):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_length, episode_return = 0, 0.0 \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_length += 1\n",
    "        episode_return += reward\n",
    "        env.render()\n",
    "        print()\n",
    "    return info, episode_length, episode_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-journalist",
   "metadata": {},
   "source": [
    "### 1.1 Warm-up: único cliente, poucos produtos\n",
    "\n",
    "Podemos passar `user_id` como argumento chave para construir um simulador das visitas de um único cliente. Isso facilita a inspeção do ambiente e serve como um caso base para testarmos estratégias simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "env1, _, test_env1 = make_shopping_cart_envs(max_products=5, user_id=54)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-homeless",
   "metadata": {},
   "source": [
    "Os espaços de estado e ação tem tamanho em função do número de produtos (`max_products`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "env1.observation_space, env1.action_space, env1.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-exhibition",
   "metadata": {},
   "source": [
    "Podemos ver que as ações são vetores binários indicando quais produtos recomendar para o carrinho do usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "env1.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-abraham",
   "metadata": {},
   "source": [
    "Como \"sanity check\", podemos verificar se a ação amostrada está no espaço de ações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert env1.action_space.sample() in env1.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_random_episode(env1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-anger",
   "metadata": {},
   "source": [
    "Por fim, verificamos o desempenho médio da política uniforme em 100 episódios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "eval_random_returns(env1, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "info, episode_length, episode_return = test_random_agent(test_env1)\n",
    "pprint(info)\n",
    "print(f\"episode_length = {episode_length}, episode_return = {episode_return:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-malpractice",
   "metadata": {},
   "source": [
    "### 1.2 Problema completo: vários clientes, mais produtos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "env2, _, test_env2 = make_shopping_cart_envs(max_products=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "eval_random_returns(env2, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "info, episode_length, episode_return = test_random_agent(test_env2)\n",
    "pprint(info)\n",
    "print(f\"episode_length = {episode_length}, episode_return = {episode_return:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-hardware",
   "metadata": {},
   "source": [
    "## 2. Escolhendo o algoritmo\n",
    "\n",
    "- Dentre os algoritmos estudados em aula, qual o mais adequado? (A) DQN ou (B) SAC?\n",
    "- É necessário alterar o algoritmo de alguma forma?\n",
    "\n",
    "\n",
    "- Qual a especificação das redes (entrada e saída)? Que tipo de arquiterura (linear, linear + hand-engineered features, conv nets, MLP, ...)\n",
    "- Comparação: redes independentes vs. features compartilhadas entre as predições de probabilidade de compra de cada item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-hardwood",
   "metadata": {},
   "source": [
    "## Q-Network\n",
    "\n",
    "<img src=\"img/arch.png\" alt=\"Qnet\" style=\"width: 500px;\"/>\n",
    "\n",
    "Nossa arquitetura implementa a seguinte função $Q_{\\phi} \\colon S \\mapsto \\mathbb{R}^{2 |A|}$, ilustrada acima.\n",
    "\n",
    "Note que decompomos o valor de uma ação (_slate_ de recomendações) em valores de recomendar ou não cada produto ao cliente.\n",
    "\n",
    "O cálculo do valor do _slate_ final é a soma dos valores estimados de cada recomendação.\n",
    "$$\n",
    "Q_\\phi(s, a) = \\sum_k q_{\\phi_k}(s, a_k)\n",
    "$$\n",
    "\n",
    "Através dessa implementação, estamos desconsiderando a influência do valor de um produto com o outro na hora da recomendação.\n",
    "\n",
    "> É razoável essa fatoração? Formule argumentos a favor/contra essa decisão dependendo de sua posição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(snt.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, **config):\n",
    "        super().__init__(name=config.get(\"name\", \"QNetwork\"))\n",
    "\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self._torso = None\n",
    "        if config.get(\"layers\"):\n",
    "            self._torso = snt.nets.MLP(\n",
    "                config[\"layers\"],\n",
    "                activation=tf.nn.relu,\n",
    "                activate_final=True,\n",
    "                w_init=initializers.he_initializer(),\n",
    "                name=\"MLP\"\n",
    "            ) \n",
    "\n",
    "        self._q_values = snt.Linear(2 * action_space.n, name=\"QValues\")\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, obs):\n",
    "        \"\"\"Calcula os Q-values de todas as ações para uma dada `obs`.\"\"\"\n",
    "        h = obs\n",
    "        if self._torso:\n",
    "            h = self._torso(h)\n",
    "        return self._q_values(h)\n",
    "\n",
    "    @tf.function\n",
    "    def action_values(self, obs, actions):\n",
    "        \"\"\"Calcula os Q-values de uma única `action` específica para uma dada `obs`.\"\"\"\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "\n",
    "        batch_size = tf.shape(obs)[0]\n",
    "        actions_mask = tf.reshape(tf.one_hot(actions, depth=2), (batch_size, -1))\n",
    "        tf.assert_equal(tf.shape(actions_mask), (batch_size, 2 * self.action_space.n))\n",
    "\n",
    "        q_values = self(obs)\n",
    "        tf.assert_equal(tf.shape(q_values), (batch_size, 2 * self.action_space.n))\n",
    "\n",
    "        q_values = tf.reduce_sum(q_values * actions_mask, axis=-1)\n",
    "        tf.assert_equal(tf.shape(q_values), (batch_size,))\n",
    "\n",
    "        return q_values\n",
    "\n",
    "    @tf.function\n",
    "    def hard_update(self, other):\n",
    "        \"\"\"Copia os parâmetros da rede `other` para a rede do objeto.\"\"\"\n",
    "        for self_var, other_var in zip(self.trainable_variables, other.trainable_variables):\n",
    "            self_var.assign(other_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = QNetwork(env1.observation_space, env1.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = q_net(env1.observation_space.sample()[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in q_net.trainable_variables:\n",
    "    print(variable.name, variable.shape, variable.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "obs = np.vstack([env1.observation_space.sample() for _ in range(batch_size)])\n",
    "action = np.vstack([env1.action_space.sample() for _ in range(batch_size)])\n",
    "action.shape, action.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = q_net(obs)\n",
    "assert q_values.shape == (batch_size, 2 * env1.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = q_net.action_values(obs, action)\n",
    "assert q_values.shape == (batch_size,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-syndrome",
   "metadata": {},
   "source": [
    "## Policy $\\epsilon$-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "\n",
    "    def __init__(self, q_net, start_val=1.0, end_val=0.01, start_step=1_000, end_step=10_000):\n",
    "        self.q_net = q_net\n",
    "\n",
    "        self._schedule = schedule.PiecewiseLinearSchedule((start_step, start_val), (end_step, end_val))\n",
    "\n",
    "        self._step = tf.Variable(0., dtype=tf.float32, name=\"step\")\n",
    "        self._epsilon = tf.Variable(start_val, dtype=tf.float32, name=\"epsilon\")\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        \"\"\"Retorna ação aleatória com probabilidade epsilon, c.c., retorna ação gulosa.\"\"\"\n",
    "        self._epsilon.assign(self._schedule(self._step))\n",
    "        self._step.assign_add(1)\n",
    "        \n",
    "        d = tf.random.uniform(minval=0.0, maxval=1.0, shape=(1,))\n",
    "        #print(f\"epsilon = {float(self._epsilon):.3f}, d = {float(d):.3f}, \", end=\"\")\n",
    "        if self._epsilon > d:\n",
    "            action = tf.random.uniform(minval=0.0, maxval=1.0, shape=(self.q_net.action_space.n,))\n",
    "            action = action >= 0.5\n",
    "            #print(\"~~ random ~~ :\", end=\"\")\n",
    "        else:\n",
    "            q_values = self.q_net(tf.expand_dims(obs, axis=0))\n",
    "            q_values = tf.reshape(q_values, (-1, 2))\n",
    "            action = tf.argmax(q_values, axis=-1)\n",
    "            #print(\"!! GREEDY !! :\", end=\"\")\n",
    "\n",
    "        action = tf.cast(action, self.q_net.action_space.dtype)\n",
    "        #print(action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = EpsilonGreedyPolicy(q_net, start_val=1.0, end_val=0.1, start_step=10, end_step=50)\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"i = {i:2d} => \", end=\"\")\n",
    "    action = policy(env1.observation_space.sample()).numpy()\n",
    "    assert action.shape == (env1.action_space.n,)\n",
    "    assert action in env1.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-regard",
   "metadata": {},
   "source": [
    "### Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_double_q_learning_loss(q_net, target_q_net, gamma=0.99):\n",
    "    \"\"\"Recebe a rede online `q_net` e a rede `target_q_net` e devolve o loss function do Double Q-Learning.\"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def _loss(batch):\n",
    "        \"\"\"Recebe um batch de experiências e devolve o valor da função objetivo para esse batch.\"\"\"\n",
    "        obs = batch[\"obs\"]\n",
    "        actions = batch[\"action\"]\n",
    "        rewards = batch[\"reward\"]\n",
    "        next_obs = batch[\"next_obs\"]\n",
    "        terminals = tf.cast(batch[\"terminal\"], tf.float32)\n",
    "\n",
    "        batch_size = tf.shape(obs)[0]\n",
    "\n",
    "        # predictions\n",
    "        q_values = q_net.action_values(obs, actions)\n",
    "\n",
    "        # targets\n",
    "        next_q_values = tf.reshape(q_net(next_obs), (batch_size, -1, 2))\n",
    "        next_actions = tf.argmax(next_q_values, axis=-1, output_type=tf.int32)\n",
    "        target_next_q_values = target_q_net.action_values(next_obs, next_actions)\n",
    "        q_targets = tf.stop_gradient(rewards + (1 - terminals) * gamma * target_next_q_values)\n",
    "\n",
    "        # loss = tf.reduce_mean((q_values - q_targets) ** 2)\n",
    "        loss = tf.losses.huber(q_values, q_targets)\n",
    "        return loss\n",
    "\n",
    "    return _loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_update_fn(loss_fn, trainable_variables, learning_rate=1e-3):\n",
    "    optimizer = snt.optimizers.Adam(learning_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def _update_fn(batch):\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(trainable_variables)\n",
    "            loss = loss_fn(batch)\n",
    "\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply(grads, trainable_variables)\n",
    "\n",
    "        grads_and_vars = {var.name: (grad, var) for grad, var in zip(grads, trainable_variables)}\n",
    "\n",
    "        return loss, grads_and_vars\n",
    "\n",
    "    return _update_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-traveler",
   "metadata": {},
   "source": [
    "### DDQN - Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN:\n",
    "\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.q_net = QNetwork(self.observation_space, self.action_space, **config[\"q_net\"])\n",
    "        self.target_q_net = QNetwork(self.observation_space, self.action_space, **config[\"q_net\"])\n",
    "\n",
    "        self.policy = EpsilonGreedyPolicy(self.q_net, **config[\"policy\"])\n",
    "\n",
    "        self._ckpt_dir = config[\"checkpoint_dir\"]\n",
    "        self._ckpt = tf.train.Checkpoint(q_net=self.q_net)\n",
    "        self._ckpt_manager = tf.train.CheckpointManager(self._ckpt, directory=self._ckpt_dir, max_to_keep=1)\n",
    "\n",
    "        self._step = tf.Variable(0, dtype=tf.int32, name=\"step\")\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Cria as variáveis das redes online e target e sincroniza inicialmente.\"\"\"\n",
    "        input_spec = tf.TensorSpec(self.observation_space.shape, dtype=tf.float32)\n",
    "        tf_utils.create_variables(self.q_net, input_spec)\n",
    "        tf_utils.create_variables(self.target_q_net, input_spec)\n",
    "        self.target_q_net.hard_update(self.q_net)\n",
    "\n",
    "    def compile(self):\n",
    "        \"\"\"Compila a Double DQN loss junto com a DuelingQNetwork.\"\"\"\n",
    "        self.update_learner = make_update_fn(\n",
    "            make_double_q_learning_loss(self.q_net, self.target_q_net, gamma=self.config[\"q_net\"][\"gamma\"]),\n",
    "            self.q_net.trainable_variables,\n",
    "            learning_rate=self.config[\"q_net\"][\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "    def step(self, obs, training=True):\n",
    "        \"\"\"Escolhe a ação para a observação dada.\"\"\"\n",
    "        obs = tf.convert_to_tensor(obs, dtype=tf.float32)\n",
    "        if training:\n",
    "            action = self.policy(obs)\n",
    "        else:\n",
    "            q_values = self.q_net(tf.expand_dims(obs, axis=0))\n",
    "            q_values = tf.reshape(q_values, (-1, 2))\n",
    "            action = tf.argmax(q_values, axis=-1)\n",
    "        return action.numpy()\n",
    "\n",
    "    def learn(self, batch):\n",
    "        \"\"\"Recebe um batch de experiências, atualiza os parâmetros das redes, e devolve algumas métricas.\"\"\"\n",
    "        loss, grads_and_vars = self.update_learner(batch)\n",
    "\n",
    "        # update target network\n",
    "        self._step.assign_add(1)\n",
    "        if self._step % self.config[\"q_net\"][\"target_update_freq\"] == 0:\n",
    "            self.target_q_net.hard_update(self.q_net)\n",
    "\n",
    "        stats = {\n",
    "            \"loss\": loss,\n",
    "            \"epsilon\": self.policy._epsilon,\n",
    "            \"vars\": {key: variable for key, (_, variable) in grads_and_vars.items()},\n",
    "            \"grads\": {f\"grad_{key}\": grad for key, (grad, _) in grads_and_vars.items()},\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Salva o estado atual do agente (i.e., o valor dos parâmetros da rede online) nesse momento.\"\"\"\n",
    "        return self._ckpt_manager.save()\n",
    "\n",
    "    def restore(self, save_path=None):\n",
    "        \"\"\"Carrega o último checkpoint salvo anteriormente no `save_path`.\"\"\"\n",
    "        if not save_path:\n",
    "            save_path = self._ckpt_manager.latest_checkpoint\n",
    "        return self._ckpt.restore(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-prerequisite",
   "metadata": {},
   "source": [
    "### Protocolo de treinamento, avaliação e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    agent,\n",
    "    env,\n",
    "    eval_env,\n",
    "    replay,\n",
    "    logger,\n",
    "    total_timesteps=5000,\n",
    "    learning_starts=500,\n",
    "    learn_every=1,\n",
    "    evaluation_freq=300\n",
    "):  \n",
    "    timesteps = 0\n",
    "    episodes = 0\n",
    "    episode_returns = deque(maxlen=20)\n",
    "\n",
    "    best_episode_reward_mean = -np.inf\n",
    "    \n",
    "    with trange(total_timesteps, desc=\"training\") as pbar:\n",
    "\n",
    "        while timesteps < total_timesteps:\n",
    "            episode_return = 0.0\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "\n",
    "            train_stats, eval_stats = {}, {}\n",
    "\n",
    "            for episode_length in itertools.count():\n",
    "\n",
    "                # collect\n",
    "                action = agent.step(obs, training=True)\n",
    "                next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "                timesteps += 1\n",
    "                episode_return += reward\n",
    "\n",
    "                # add experience to replay buffer\n",
    "                replay.add(obs, action, reward, done, next_obs)\n",
    "\n",
    "                # training\n",
    "                if timesteps >= learning_starts and timesteps % learn_every == 0:\n",
    "                    batch = replay.sample()\n",
    "                    train_stats = agent.learn(batch)\n",
    "\n",
    "                # evaluation\n",
    "                if timesteps % evaluation_freq == 0:\n",
    "                    eval_stats = evaluate(agent, eval_env)\n",
    "\n",
    "                    # logging\n",
    "                    train_stats[\"episode_return_mean\"] = np.mean(episode_returns)\n",
    "                    logger.log(timesteps, train_stats, label=\"train\") \n",
    "                    logger.log(timesteps, eval_stats, label=\"evaluation\")\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                obs = next_obs\n",
    "\n",
    "            episodes += 1\n",
    "            episode_returns.append(episode_return)\n",
    "\n",
    "            # logging\n",
    "            stats = {\n",
    "                \"episodes\": episodes,\n",
    "                \"episode_length\": episode_length,\n",
    "                \"episode_return\": episode_return,\n",
    "            }\n",
    "            logger.log(timesteps, stats, label=\"collect\")\n",
    "            logger.flush()\n",
    "\n",
    "            pbar.update(episode_length)\n",
    "            pbar.set_postfix(timesteps=timesteps, episodes=episodes, avg_returns=np.mean(episode_returns) if episode_returns else None)\n",
    "\n",
    "    # final evaluation\n",
    "    stats = evaluate(agent, eval_env)\n",
    "    logger.log(timesteps, stats, label=\"evaluation\")\n",
    "    logger.flush()\n",
    "    \n",
    "    # checkpoint\n",
    "    agent.save()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, episodes=20):\n",
    "    episode_lengths, episode_returns = [], []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        episode_length, episode_return = 0, 0.0\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.step(obs, training=False)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            episode_length += 1\n",
    "            episode_return += reward\n",
    "        episode_lengths.append(episode_length)\n",
    "        episode_returns.append(episode_return)\n",
    "\n",
    "    return {\n",
    "        \"episode_return_mean\": np.mean(episode_returns),\n",
    "        \"episode_return_min\": np.min(episode_returns),\n",
    "        \"episode_return_max\": np.max(episode_returns),\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, env, episodes=3):\n",
    "    stats = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        episode_length, episode_return = 0, 0.0\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.step(obs, training=False)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_length += 1\n",
    "            episode_return += reward\n",
    "            env.render()\n",
    "        \n",
    "        stats.append({\n",
    "            \"info\": info,\n",
    "            \"episode_length\": episode_length,\n",
    "            \"episode_return\": episode_return            \n",
    "        })\n",
    "\n",
    "    env.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-confusion",
   "metadata": {},
   "source": [
    "### Busca de hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --reload_interval 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "gz_file = pathlib.Path(\"instacart_online_grocery_shopping_2017_05_01.tar.gz\")\n",
    "\n",
    "# single-user\n",
    "#env, eval_env, test_env = make_shopping_cart_envs(max_products=20, user_id=54)\n",
    "\n",
    "# multiple-users\n",
    "env, eval_env, test_env = make_shopping_cart_envs(max_products=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 5000\n",
    "\n",
    "def run_experiment(config):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n",
    "    run_id = osp.join(f\"ddqn-{env.spec.id}-{config['tags']}\".lower(), timestamp, f\"trial={config['trial_id']}\")\n",
    "    config[\"checkpoint_dir\"] = f\"ckpt/{run_id}\"\n",
    "\n",
    "    logger = logging.TFLogger(run_id, base_dir=\"logs\")\n",
    "\n",
    "    buffer = replay.ReplayBuffer(env.observation_space, env.action_space, max_size=total_timesteps, batch_size=config[\"q_net\"][\"batch_size\"])\n",
    "    buffer.build() \n",
    "\n",
    "    agent = DDQN(env.observation_space, env.action_space, config)\n",
    "    agent.build()\n",
    "    agent.compile()\n",
    "\n",
    "    stats = train(agent, env, eval_env, buffer, logger, total_timesteps=total_timesteps)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials(configs, trials=5):\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for trial_id in range(trials):\n",
    "        configs = [{**config, **{\"trial_id\": trial_id}} for config in configs]\n",
    "\n",
    "        with mp.Pool(10) as p:\n",
    "            run_stats = p.map(run_experiment, configs)\n",
    "\n",
    "        for config, stats in zip(configs, run_stats):\n",
    "            results[config[\"tags\"]].append(stats)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-ultimate",
   "metadata": {},
   "source": [
    "### Experimento 1: taxa de aprendizado vs. batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    \"q_net\": {\n",
    "        \"layers\": [],\n",
    "        \"gamma\": 0.99,\n",
    "        \"target_update_freq\": 500,\n",
    "        \"learning_rate\": 5e-3,\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "    \"policy\": {\n",
    "        \"start_val\": 1.0,\n",
    "        \"end_val\": 0.1,\n",
    "        \"start_step\": 1_000,\n",
    "        \"end_step\": 2_000\n",
    "    },\n",
    "}\n",
    "\n",
    "params_names = [\"bs\", \"lr\"]\n",
    "batch_sizes = [16, 64, 256]\n",
    "learning_rates = [1e-3, 5e-3, 1e-2]\n",
    "\n",
    "configs = []\n",
    "\n",
    "for params in itertools.product(batch_sizes, learning_rates):\n",
    "    batch_size, learning_rate = params\n",
    "\n",
    "    config = copy.deepcopy(base_config)\n",
    "    config[\"q_net\"].update({\"learning_rate\": learning_rate, \"batch_size\": batch_size})\n",
    "\n",
    "    tags = \",\".join([f\"{name}={value}\" for name, value in zip(params_names, params)])\n",
    "    config[\"tags\"] = tags\n",
    "    \n",
    "    configs.append(config)\n",
    "\n",
    "results = run_trials(configs, trials=3)\n",
    "\n",
    "for config in configs:\n",
    "    tags = config['tags']\n",
    "    episode_return_means = [stats[\"episode_return_mean\"] for stats  in results[tags]]\n",
    "    print(f\"{tags:20} : {episode_return_means}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-auckland",
   "metadata": {},
   "source": [
    "### Experimento 2: epsilon-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    \"q_net\": {\n",
    "        \"layers\": [],\n",
    "        \"gamma\": 0.99,\n",
    "        \"target_update_freq\": 500,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 256\n",
    "    },\n",
    "    \"policy\": {\n",
    "        \"start_val\": 1.0,\n",
    "        \"end_val\": 0.1,\n",
    "        \"start_step\": 1_000,\n",
    "        \"end_step\": 2_000\n",
    "    },\n",
    "}\n",
    "\n",
    "params_names = [\"start\", \"end\"]\n",
    "start_steps = [100, 500, 1000]\n",
    "end_steps = [500, 1000, 2000, 4000]\n",
    "\n",
    "configs = []\n",
    "\n",
    "for params in itertools.product(start_steps, end_steps):\n",
    "    start_step, end_step = params\n",
    "    if start_step >= end_step:\n",
    "        continue\n",
    "\n",
    "    config = copy.deepcopy(base_config)\n",
    "    config[\"policy\"].update({\"start_step\": start_step, \"end_step\": end_step})\n",
    "\n",
    "    tags = \",\".join([f\"{name}={value}\" for name, value in zip(params_names, params)])\n",
    "    config[\"tags\"] = tags\n",
    "\n",
    "    configs.append(config)\n",
    "\n",
    "results = run_trials(configs, trials=3)\n",
    "\n",
    "for config in configs:\n",
    "    tags = config['tags']\n",
    "    episode_return_means = [stats[\"episode_return_mean\"] for stats  in results[tags]]\n",
    "    print(f\"{tags:20} : {episode_return_means}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-parameter",
   "metadata": {},
   "source": [
    "### Experimento 3: arquitetura da Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    \"q_net\": {\n",
    "        \"layers\": [],\n",
    "        \"gamma\": 0.99,\n",
    "        \"target_update_freq\": 500,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 256\n",
    "    },\n",
    "    \"policy\": {\n",
    "        \"start_val\": 1.0,\n",
    "        \"end_val\": 0.1,\n",
    "        \"start_step\": 500,\n",
    "        \"end_step\": 2_000\n",
    "    },\n",
    "}\n",
    "\n",
    "params_names = [\"units\"]\n",
    "sizes = [8, 16, 32, 64, 256]\n",
    "\n",
    "configs = []\n",
    "\n",
    "for size in sizes:\n",
    "    config = copy.deepcopy(base_config)\n",
    "    config[\"q_net\"].update({\"layers\": [size] * 2})\n",
    "    config[\"tags\"] = f\"size={size}\"\n",
    "    configs.append(config)\n",
    "\n",
    "results = run_trials(configs, trials=3)\n",
    "\n",
    "for config in configs:\n",
    "    tags = config['tags']\n",
    "    episode_return_means = [stats[\"episode_return_mean\"] for stats  in results[tags]]\n",
    "    print(f\"{tags:20} : {episode_return_means}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-questionnaire",
   "metadata": {},
   "source": [
    "## 3. Análise da solução\n",
    "\n",
    "Qualitativamente como podemos avaliar o desempenho da solução obtida?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"q_net\": {\n",
    "        \"layers\": [256, 256],\n",
    "        #layers\": [],\n",
    "        \"gamma\": 0.99,\n",
    "        \"target_update_freq\": 500,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 256\n",
    "    },\n",
    "    \"policy\": {\n",
    "        \"start_val\": 1.0,\n",
    "        \"end_val\": 0.1,\n",
    "        \"start_step\": 500,\n",
    "        \"end_step\": 2_000\n",
    "    },\n",
    "    \"checkpoint_dir\": \"ckpt/\"\n",
    "}\n",
    "\n",
    "checkpoint_dir = \"ckpt/ddqn-shoppingcart-v0-user_id=54-size=256/2021-02-04-18:27/trial=2/ckpt-1\"\n",
    "agent = DDQN(env.observation_space, env.action_space, config)\n",
    "agent.build()\n",
    "agent.restore(checkpoint_dir).assert_consumed()\n",
    "\n",
    "stats = test(agent, test_env, episodes=1)\n",
    "pprint(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-booth",
   "metadata": {},
   "source": [
    "## 4. Decisões de modelagem - MDPs (caixa-branca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-storage",
   "metadata": {},
   "source": [
    "**Simulador não-paramétrico:**\n",
    "- Episódio: replay do log de transações do cliente (desenrolar a série histórica)\n",
    "- Como dados históricos podem ser encapsulados como função de transição (replay do log de pedidos do cliente, ...)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-exception",
   "metadata": {},
   "source": [
    "**Modelagem do MDP**\n",
    "- Timestep (estágio de decisão) != noção de tempo => entre decisões intervalo de tempo diferentes\n",
    "- env.reset(): múltiplos clientes => distribuição inicial $\\rho$ => escolhe novo cliente para o episódio\n",
    "- Função de transição: ação hoje não altera a tomada de decisão amanhã => o sistema não tem memória / stateless!\n",
    "- Função de recompensa = tp - tn (feedback multi-dimensional vs. recompensa escalar) => incentivar mais produtos no slate com punição para produtos não escolhidos\n",
    "- observabilidade parcial: timestamp da sessão + recency (compra)\n",
    "- Espaço de ações tem tamanho fixo independente do cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-cattle",
   "metadata": {},
   "source": [
    "**Sistemas de recomendação:**\n",
    "- Tipicamente em sistemas de recomendação, as observações são compostas por no mínimo: \n",
    "    - perfil do cliente,\n",
    "    - propensão de compra (cliente, produto), \n",
    "    - estatísticas de compras passadas do cliente, \n",
    "    - estatísticas de sucesso de cada produto (agregação das preferências dos usuários),\n",
    "    - retorno acumulado como estatística de sucesso das recomendaçoes passadas para aquele cliente => sinal para aumentar ou diminuir a exploração"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-strengthening",
   "metadata": {},
   "source": [
    "**Interpretação/análise da política:**\n",
    "- política para 1 cliente => treinar demais em 1 cliente => overfitting/memorização => modelo \"auto-regressivo\" do histórico do cliente\n",
    "- política para n clientes => modelo aprende política do cliente médio <=> tal produto vai bem em tal dia e horário, ou qual produto é mais vendável\n",
    "- como customizar a política aprendida para cada cliente?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-bloom",
   "metadata": {},
   "source": [
    "## 5. Considerações finais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-separate",
   "metadata": {},
   "source": [
    "**Modelagem do Problema:**\n",
    "- Simulador => 1a opção, ponto de partida\n",
    "- Efeitos de longo prazo importantes de se considear: por exemplo, errar muitas recomendações diminui o engajamento do cliente no futuro? recomendar muitos produtos pode ser detrimental limitações de UI? \n",
    "- Representação parcial do estado implica em necessidade de memória por parte do agente (e.g., RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-assets",
   "metadata": {},
   "source": [
    "**Algoritmos:**\n",
    "- Nesse curso introdutório focamos em RL online com \"simuladores\" para apresentar os conceitos básicos de RL...\n",
    "- No entanto, em aplicações reais (e-commerce, recomendadores, ...) => Off-line (batch) RL pode ser uma melhor solução!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
